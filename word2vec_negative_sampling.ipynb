{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "word2vec negative sampling.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "XklA_iwHUlLt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -I --no-cache-dir pillow"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iGL7DOWsVTa8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "from matplotlib import pyplot as plt\n",
        "import collections #for counter\n",
        "is_cuda = torch.cuda.is_available\n",
        "tensor_to_numpy = lambda t:t.detach().cpu().numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dylKOm_qVeCJ",
        "colab_type": "text"
      },
      "source": [
        "Get the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bbPT5ZuWVlen",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget https://s3.amazonaws.com/video.udacity-data.com/topher/2018/October/5bbe6499_text8/text8.zip \n",
        "!unzip text8.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7hpGN1cUVmUl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('text8','rb') as f:\n",
        "    text = f.read()\n",
        "text = text.lower()\n",
        "text = text.split()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SUxDamh-V7qH",
        "colab_type": "text"
      },
      "source": [
        "1. remove rare words\n",
        "2. make mappings to and from integer\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M2eVQg6NWXIv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def remove_rare(text,thresh_freq=5):\n",
        "    \n",
        "    word_counts = collections.Counter(text)\n",
        "    text = [w for w in text if word_counts[w] > thresh_freq]\n",
        "    return text\n",
        "\n",
        "text = remove_rare(text)\n",
        "print(text[:100])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lGSUXu77ZMyy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def word_integer_mappings(text):\n",
        "\n",
        "    word_counts = collections.Counter(text)\n",
        "    int_to_word = sorted(word_counts,\n",
        "                         key=word_counts.get,\n",
        "                        reverse=True)\n",
        "#     int_to_word = list(word_counts.keys())\n",
        "    word_to_int = {w:i for i,w in enumerate(int_to_word)}\n",
        "    return int_to_word,word_to_int\n",
        "int_to_word,word_to_int = word_integer_mappings(text)\n",
        "print(int_to_word[:10],\n",
        "     list(word_to_int.keys())[:10])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gwckDGlsb6YZ",
        "colab_type": "text"
      },
      "source": [
        "subsample extremely frequent words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wl3v-cv-clnU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def subsample(text,thresh=1e-5):\n",
        "    nwords = len(text)\n",
        "    word_counts = collections.Counter(text)\n",
        "    subsampled = [w for w in text if np.random.random() < 1 - np.sqrt(thresh * nwords/word_counts[w])]\n",
        "    return subsampled\n",
        "print(len(text))\n",
        "subsampled = subsample(text)\n",
        "print(len(subsampled))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1l0PCyhgfKYZ",
        "colab_type": "text"
      },
      "source": [
        "Make indexed representation for training words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5rpHXd_eTxF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_words = [word_to_int[w] for w in subsampled]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1B2ch3EMfNHz",
        "colab_type": "text"
      },
      "source": [
        "1. Make contexts for every word \n",
        "2. make training batches"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11IQDse7fT8s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_target(text,\n",
        "               idx,\n",
        "               max_context_window = 5):\n",
        "    context_window = np.random.randint(1,max_context_window+1)\n",
        "    text_len = len(text)\n",
        "#     print(idx,context_window,text_len)\n",
        "    ix_min = max(idx-context_window,0)\n",
        "    ix_max = min(idx+context_window,text_len)\n",
        "    \n",
        "    context = text[ix_min:idx] + text[idx+1:ix_max+1]\n",
        "#     print(context.__len__())\n",
        "    return list(context)\n",
        "\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OSJBFdZrmHye",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_batches(text,\n",
        "                batch_size,\n",
        "                max_context_window = 5):\n",
        "    nwords = len(text)\n",
        "    nbatches = (nwords + batch_size - 1)//batch_size\n",
        "\n",
        "    for bix in range(nbatches):\n",
        "        batchx = []\n",
        "        batchy = []\n",
        "\n",
        "        for idx in range(batch_size*bix,min(batch_size*(bix+1),nwords)):\n",
        "#             print(idx)\n",
        "\n",
        "            ctx = make_target(text,\n",
        "                             idx,\n",
        "                             max_context_window = max_context_window)\n",
        "            batchy.extend(ctx)\n",
        "            batchx.extend([text[idx]]*len(ctx))\n",
        "        yield batchx,batchy\n",
        "if True:\n",
        "    ''' Testing the function '''\n",
        "    int_text = [i for i in range(20)]\n",
        "    x,y = next(make_batches(int_text, batch_size=4, max_context_window=5))\n",
        "#     x,y = make_batches(int_text, batch_size=4, max_context_window=5)\n",
        "\n",
        "    print('x\\n', x)\n",
        "    print('y\\n', y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQn39s1HtW7g",
        "colab_type": "text"
      },
      "source": [
        "Define the Negatively Sampled SkipGram Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B6zhuXwStCr6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SkipGramNeg(torch.nn.Module):\n",
        "    def __init__(self,vocab_len,embed_len):\n",
        "        super(SkipGramNeg,self).__init__()\n",
        "        self.vocab_len = vocab_len\n",
        "        self.embed_len = embed_len\n",
        "        \n",
        "        self.in_embed = torch.nn.Embedding(vocab_len,\n",
        "                                           embed_len)\n",
        "        \n",
        "        self.out_embed = torch.nn.Embedding(vocab_len,\n",
        "                                            embed_len)\n",
        "        \n",
        "        self.in_embed.weight.data.uniform_(-1,1)\n",
        "        self.out_embed.weight.data.uniform_(-1,1)\n",
        "    def forward_input(self,x):\n",
        "        return self.in_embed(x)\n",
        "    def forward_output(self,x):\n",
        "        return self.out_embed(x)\n",
        "    def forward_noise(self,batch_size,n_samples,word_probs = None):\n",
        "        '''say n_samples = 10, batch_size =5\n",
        "        will return 10 noise words per input word\n",
        "        i.e. returns a 10x5 tensor'''\n",
        "        if word_probs == None:\n",
        "            word_probs = torch.ones(self.vocab_len,)\n",
        "        noise_idx = torch.multinomial(word_probs,\n",
        "                                       batch_size*n_samples,\n",
        "                                       replacement=True)\n",
        "        if is_cuda:\n",
        "            noise_idx = noise_idx.cuda()\n",
        "        noise_embed = self.out_embed(noise_idx).view(batch_size,n_samples,self.embed_len)\n",
        "        return noise_embed\n",
        "        \n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78B8OzCsy668",
        "colab_type": "text"
      },
      "source": [
        "Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jk7YfC1Hltom",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_words.__len__()\n",
        "# next(make_batches(train_words,batch_size))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0JhNmTV-1rEh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embed_len = 300\n",
        "model = SkipGramNeg(len(int_to_word),embed_len)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kHiuvAXqyiUC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tqdm\n",
        "opt = torch.optim.Adam(model.parameters(),lr=3e-3)\n",
        "if is_cuda:\n",
        "    model.cuda()\n",
        "    pass\n",
        "nepochs = 10\n",
        "batch_size = 512\n",
        "nwords = len(train_words)\n",
        "nbatches = (nwords + batch_size - 1)//batch_size\n",
        "\n",
        "nnoise = 5\n",
        "vis_every = nepochs - 1\n",
        "nvalid = 16\n",
        "trends = {'loss':[]}\n",
        "for e in tqdm.tqdm_notebook(range(nepochs)):\n",
        "    for bi,(bx,by) in enumerate(tqdm.tqdm_notebook(make_batches(train_words,batch_size),total = nbatches)):\n",
        "        ''' Make the embeddings for the input, context and the noise '''\n",
        "        \n",
        "        bx,by = torch.LongTensor(bx),torch.LongTensor(by)\n",
        "#         print(bi,\n",
        "#               bx.shape)\n",
        "        if is_cuda:\n",
        "            bx = bx.cuda()\n",
        "            by = by.cuda()\n",
        "            pass\n",
        "        bx_embed = model.forward_input(bx)\n",
        "        by_embed = model.forward_output(by)\n",
        "        \n",
        "        noise_embed = model.forward_noise(batch_size=bx_embed.shape[0],n_samples=5)\n",
        "        \n",
        "        bx_embed = bx_embed.view(bx_embed.shape[0],embed_len)\n",
        "        by_embed = by_embed.view(by_embed.shape[0],embed_len)\n",
        "#         noise_embed = noise_embed.view(-1,embed_len)\n",
        "        ''' Negative Sampling Loss '''\n",
        "        ctx_dot = torch.einsum('ij,ij->i',[bx_embed,by_embed])\n",
        "        noise_dot = torch.einsum('bij, bkj -> bik',[bx_embed.unsqueeze(1),noise_embed])\n",
        "        noise_dot = noise_dot.unsqueeze(1)\n",
        "        \n",
        "        if 'check if the dot products are nan' and (torch.isnan(ctx_dot).sum() > 0 or torch.isnan(noise_dot).sum() > 0):\n",
        "            import pdb;pdb.set_trace()\n",
        "\n",
        "        if 'check for zeros in the dot product' and torch.any(ctx_dot == 0.) or torch.any(noise_dot == 0.):\n",
        "            import pdb;pdb.set_trace()\n",
        "        \n",
        "        # ! IMPORTANT *.sigmoid().log() does not work as well, leading to Inf.\n",
        "        # use torch.nn.functional.logsigmoid() instead\n",
        "        \n",
        "        log_sigmoid_ctx = torch.nn.functional.logsigmoid(ctx_dot)\n",
        "        log_sigmoid_noise = torch.nn.functional.logsigmoid(noise_dot)\n",
        "        \n",
        "#         log_sigmoid_ctx[torch.isinf(log_sigmoid_ctx)] = 0.\n",
        "#         log_sigmoid_noise[torch.isinf(log_sigmoid_noise)] = 0.\n",
        "        \n",
        "        pos = log_sigmoid_ctx\n",
        "        neg = log_sigmoid_noise.sum(-1)\n",
        "        loss = (pos+neg).mean()\n",
        "        \n",
        "        opt.zero_grad()\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        \n",
        "        if 'check if grad is nan' and torch.isnan(model.in_embed.weight.grad).sum() > 0:\n",
        "            import pdb;pdb.set_trace()\n",
        "        \n",
        "        trends['loss'].append(tensor_to_numpy(loss))\n",
        "        \n",
        "#         break\n",
        "    if e%vis_every == 0:\n",
        "        plt.figure()\n",
        "        plt.plot(trends['loss'])\n",
        "        plt.show()\n",
        "        \n",
        "        embed_vec = model.in_embed.weight\n",
        "        embed_vec = embed_vec/embed_vec.pow(2).sum(-1).sqrt().unsqueeze(-1)\n",
        "        valid_idx = np.random.randint(0,\n",
        "                                      100,\n",
        "                                      size=(nvalid//2,))\n",
        "        valid_idx = np.append(valid_idx,\n",
        "                          np.random.randint(1000,\n",
        "                                            1000+100,\n",
        "                                            size=(nvalid//2,)))\n",
        "        valid_idx = torch.LongTensor(valid_idx)\n",
        "        if is_cuda:\n",
        "            valid_idx.cuda()\n",
        "        valid_embed = embed_vec[valid_idx]\n",
        "        similarities = torch.einsum('ij,kj->ik',[valid_embed,embed_vec])\n",
        "        \n",
        "#         valid_idx_,sim_ = tensor_to_numpy(valid_idx),tensor_to_numpy(similarities)\n",
        "        for v,s in zip(valid_idx,\n",
        "                       similarities):\n",
        "            \n",
        "            w = int_to_word[tensor_to_numpy(v)].decode('utf-8')\n",
        "            \n",
        "            top_s = s.topk(6)[-1]\n",
        "            top_s = [int_to_word[s_i].decode('utf-8') for s_i in tensor_to_numpy(top_s)]\n",
        "  \n",
        "            \n",
        "            print(w + ':' + ' '.join(top_s))\n",
        "        print('-'*20)\n",
        "        pass\n",
        "    \n",
        "#     break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nz-shZFB2cq-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "noise_embed = model.forward_noise(batch_size=bx_embed.shape[0],n_samples=5)\n",
        "bx_embed.unsqueeze(1).shape,noise_embed.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gT2I1XFy0j7E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.in_embed.weight"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}