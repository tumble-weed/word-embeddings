{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "word2vec skipgram.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "dp0DqyqF1YQo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install --no-cache-dir -I pillow"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PFg6t_y515wE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#get dataset\n",
        "!wget https://s3.amazonaws.com/video.udacity-data.com/topher/2018/October/5bbe6499_text8/text8.zip\n",
        "!unzip text8.zip\n",
        "!ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xQjIO9qn2CZy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "with open('text8','r') as f:\n",
        "    text = f.read()\n",
        "print(text[:100])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UW-8y8Hg1oYF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "matplotlib.rcParams['axes.grid'] = False\n",
        "from matplotlib import pyplot as plt\n",
        "is_cuda = torch.cuda.is_available\n",
        "import tqdm\n",
        "tensor_to_numpy = lambda t:t.detach().cpu().numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SY52tSBIFUqS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "d = {1:'a',\n",
        "    2:'b'}\n",
        "d,sorted(d,key=d.get)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9b4Rhsox2cET",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import collections\n",
        "def replace_punctuation_with_token(text):\n",
        "    text = text.lower()\n",
        "    tokens = {'.':'<PERIOD>',\n",
        "             ',':'<COMMA>',\n",
        "             '\"':'<QUOTATION_MARK>',\n",
        "             ';':'<SEMICOLON>',\n",
        "             '!':'<EXCLAMATION_MARK>',\n",
        "             '?':'<QUESTION_MARK>',\n",
        "             '(':'<PAREN_OPEN>',\n",
        "             ')':'<PAREN_CLOSE>',\n",
        "             '--':'<DOUBLE_DASH>',\n",
        "             ':':'<COLON>',\n",
        "             }\n",
        "    for p,t in tokens.items():\n",
        "        text.replace(p,t)\n",
        "    return text\n",
        "\n",
        "def remove_rare_words(text):\n",
        "    words = text.split()\n",
        "    counts = collections.Counter(words)\n",
        "    words = [w for w in words if counts[w] > 5]\n",
        "    return words\n",
        "\n",
        "def mappings(words):\n",
        "    counts = collections.Counter(words)\n",
        "    sorted_words = sorted(counts,\n",
        "                    key=counts.get,\n",
        "                   reverse=True)\n",
        "    \n",
        "\n",
        "    \n",
        "    idx_to_word = sorted_words\n",
        "    word_to_idx = {w:idx for idx,w in enumerate(sorted_words)}\n",
        "\n",
        "    return word_to_idx,idx_to_word\n",
        "\n",
        "text = replace_punctuation_with_token(text)\n",
        "trimmed = remove_rare_words(text)\n",
        "word_to_idx,idx_to_word = mappings(trimmed)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PbGVaM3w5YdN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "int_words = [word_to_idx[w] for w in trimmed]\n",
        "total_words = len(int_words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMPt3uwR68bN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#subsampling the words, will remove a lot of occurances of the most frequent words like 'the' , 'a', 'an' etc.\n",
        "thresh = 1e-5\n",
        "word_counts = collections.Counter(int_words)\n",
        "word_freq = {w:(c*1./total_words) for (w,c) in word_counts.items()}\n",
        "discard_prob = {w: 1. - np.sqrt(thresh/f) for (w,f) in word_freq.items()}\n",
        "tosses = np.random.random(size=(len(int_words),))\n",
        "train_words = [w for w,t in zip(int_words,tosses) if t<(1. -discard_prob[w])]\n",
        "# keep_prob = 1 - np.sqrt(thresh/word_counts)\n",
        "print('check if all words remain'\n",
        "    ,len(np.unique(int_words)),\n",
        "    len(np.unique(train_words)))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9HK6RQAt6eY-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_context(words,idx,window=5):\n",
        "    ctx_len = np.random.randint(1,window+1)\n",
        "    start = max(idx - ctx_len,0)\n",
        "    end = min(start + ctx_len,len(words))\n",
        "    context = words[start:idx] + words[idx+1:end]\n",
        "    return list(context)\n",
        "\n",
        "if False:\n",
        "    idx = 15\n",
        "    ctx = get_context(train_words,idx)\n",
        "    print(idx_to_word[idx],[idx_to_word[i] for i in ctx])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n0grjcCbDWPX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import itertools\n",
        "def get_batches(words,\n",
        "               batch_size,\n",
        "               window=5):\n",
        "    n_batches = (len(words) + batch_size -1)//batch_size\n",
        "    batch_idxs = range(n_batches)\n",
        "#     print(int_words[:10])\n",
        "    for i in batch_idxs:\n",
        "        x = words[i*batch_size:(i+1)*batch_size]\n",
        "        y = [get_context(words,xi) for xi in x]\n",
        "        x = [ [xi]*len(yi) for xi,yi in zip(x,y)]\n",
        "        x = list(itertools.chain(*x))\n",
        "        y = list(itertools.chain(*y))\n",
        "        if False:\n",
        "            print(x,y)\n",
        "            import pdb;pdb.set_trace()\n",
        "        yield x,y\n",
        "\n",
        "if False:\n",
        "    int_text = [i for i in range(20)]\n",
        "    x,y = next(get_batches(int_text, batch_size=4, window=5))\n",
        "\n",
        "    print('x\\n', x)\n",
        "    print('y\\n', y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1xcXKdnkIaDH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "Defining the model\n",
        "'''\n",
        "class SkipGram(torch.nn.Module):\n",
        "    def __init__(self,n_vocab,n_embed):\n",
        "        super(SkipGram,self).__init__()\n",
        "        self.embed = torch.nn.Embedding(n_vocab,\n",
        "                                        n_embed)\n",
        "        self.output = torch.nn.Linear(n_embed,\n",
        "                                    n_vocab)\n",
        "        self.log_softmax = torch.nn.LogSoftmax(dim=1)\n",
        "        \n",
        "        pass\n",
        "    def forward(self,x):\n",
        "        embed_x = self.embed(x)\n",
        "        scores_x = self.output(embed_x)\n",
        "        log_softmax_x = self.log_softmax(scores_x)\n",
        "        \n",
        "        return log_softmax_x\n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQgzdU86-Zj5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embed_dim = 300\n",
        "vocab_size = len(idx_to_word)\n",
        "skipgram = SkipGram(vocab_size,\n",
        "                    embed_dim)\n",
        "v = torch.tensor([1]).long()\n",
        "\n",
        "if is_cuda:\n",
        "    v = v.cuda()\n",
        "    skipgram.cuda()\n",
        "skipgram(v)    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nSJA9Z2rBIq0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nepochs = 100\n",
        "val_freq = 1\n",
        "opt = torch.optim.Adam(skipgram.parameters(),lr=3e-3)\n",
        "nvalid = 16\n",
        "freq_window = 100\n",
        "idx_to_word_np = np.array(idx_to_word)\n",
        "for e in tqdm.tqdm_notebook(range(nepochs)):\n",
        "    for w,ctx in get_batches(train_words,512):\n",
        "        w,ctx = torch.LongTensor(w),torch.LongTensor(ctx)\n",
        "        if is_cuda:\n",
        "            w = w.cuda()\n",
        "            ctx = ctx.cuda()\n",
        "        log_preds = skipgram(w)\n",
        "        loss = torch.nn.functional.nll_loss(log_preds,\n",
        "                                               ctx)\n",
        "        opt.zero_grad()\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "#         break\n",
        "        \n",
        "    if e%val_freq == 0:\n",
        "        embed_matrix = skipgram.embed.weight\n",
        "        embed_mag = embed_matrix.pow(2).sum(1).sqrt().unsqueeze(0)\n",
        "        \n",
        "        valid_words_freq = np.random.choice(np.arange(freq_window),size=nvalid//2)\n",
        "        valid_words_mid = np.random.choice(np.arange(1000,1000+freq_window),size=nvalid//2)\n",
        "        valid_words = np.concatenate([valid_words_freq,\n",
        "                                valid_words_mid])\n",
        "        valid_words = torch.tensor(valid_words).long()\n",
        "        if is_cuda:\n",
        "            valid_words = valid_words.cuda()\n",
        "        embed_of_valid = skipgram.embed(valid_words)\n",
        "        valid_similarities = torch.mm(embed_of_valid,\n",
        "                               embed_matrix.t())/embed_mag\n",
        "        _,closest_idxs = valid_similarities.topk(6)\n",
        "        valid_words,closest_idxs = tensor_to_numpy(valid_words),tensor_to_numpy(closest_idxs)\n",
        "        for ii,(w,syn) in enumerate(zip(valid_words,closest_idxs)):\n",
        "            \n",
        "            closest_words = idx_to_word_np[syn]\n",
        "            print(f'Closest word to {idx_to_word_np[w]}:{closest_words}')\n",
        "        print('-'*50)\n",
        "        pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YTExAAXeGtGL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "idx_to_word[]"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}